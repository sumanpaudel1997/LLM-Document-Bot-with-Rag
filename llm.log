2024-02-25 11:27:36,664 - INFO - main.py:37 - Ingesting The Data.
2024-02-25 11:27:36,890 - INFO - main.py:42 - Connecting to Qdrant Instance
2024-02-25 11:27:36,905 - INFO - main.py:47 - Creating the Vector indices
2024-02-25 11:27:42,168 - INFO - main.py:53 - Creating Chat Engine
2024-02-25 11:27:42,225 - INFO - main.py:65 - All OK.
2024-02-25 11:27:58,302 - INFO - _client.py:1026 - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-02-25 11:27:58,306 - INFO - main.py:72 - [('what is LSTM?', AgentChatResponse(response='LSTM stands for Long Short-Term Memory. It is a type of recurrent neural network (RNN) architecture that is designed to overcome the vanishing gradient problem in traditional RNNs. LSTM networks have the ability to learn long-term dependencies in data by maintaining a memory state and selectively updating it using gates. These gates include the input gate, forget gate, and output gate, which regulate the flow of information within the network. This architecture makes LSTM networks particularly effective for tasks involving sequential data, such as speech recognition, language modeling, and machine translation.', sources=[ToolOutput(content="system: You are a chatbot, able to have normal interactions with previous conversations, as well as talk Help to generate the given query prompt \n{query}Don't use your knowledge except for the context\nContext information is below.\n--------------------\npage_label: 9\nfile_path: data/docs/attentionisallyouneed.pdf\n\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\nsteps (dev) (dev)×106\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n(A)1 512 512 5.29 24.9\n4 128 128 5.00 25.5\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n(B)16 5.16 25.1 58\n32 5.01 25.4 60\n(C)2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n(D)0.0 5.77 24.6\n0.2 4.95 25.5\n0.0 4.67 25.3\n0.2 5.47 25.7\n(E) positional embedding instead of sinusoids 4.92 25.7\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\nresults to the base model.\n7 Conclusion\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor .\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\n9\n--------------------\n", tool_name='retriever', raw_input={'message': 'what is LSTM?'}, raw_output=ChatMessage(role=<MessageRole.SYSTEM: 'system'>, content="You are a chatbot, able to have normal interactions with previous conversations, as well as talk Help to generate the given query prompt \n{query}Don't use your knowledge except for the context\nContext information is below.\n--------------------\npage_label: 9\nfile_path: data/docs/attentionisallyouneed.pdf\n\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\nsteps (dev) (dev)×106\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n(A)1 512 512 5.29 24.9\n4 128 128 5.00 25.5\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n(B)16 5.16 25.1 58\n32 5.01 25.4 60\n(C)2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n(D)0.0 5.77 24.6\n0.2 4.95 25.5\n0.0 4.67 25.3\n0.2 5.47 25.7\n(E) positional embedding instead of sinusoids 4.92 25.7\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\nresults to the base model.\n7 Conclusion\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor .\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\n9\n--------------------\n", additional_kwargs={}))], source_nodes=[NodeWithScore(node=TextNode(id_='1c15df83-5f04-4ca3-8cf2-b0b08be7d836', embedding=None, metadata={'page_label': '9', 'file_name': 'attentionisallyouneed.pdf', 'file_path': 'data/docs/attentionisallyouneed.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2024-02-25', 'last_modified_date': '2024-02-25', 'last_accessed_date': '2024-02-25'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='1cb88c6b-49ae-42ef-b248-14224ce3bbee', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '9', 'file_name': 'attentionisallyouneed.pdf', 'file_path': 'data/docs/attentionisallyouneed.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2024-02-25', 'last_modified_date': '2024-02-25', 'last_accessed_date': '2024-02-25'}, hash='68e9a3fef5cb6315827596ca63062d20cdaf2c2f80dbe68e65947eed7daf5e6d'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='300f568f-ec58-4072-871f-a15435426275', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '8', 'file_name': 'attentionisallyouneed.pdf', 'file_path': 'data/docs/attentionisallyouneed.pdf', 'file_type': 'application/pdf', 'file_size': 569417, 'creation_date': '2024-02-25', 'last_modified_date': '2024-02-25', 'last_accessed_date': '2024-02-25'}, hash='52dd9b540bb6803a32afc0bf30b768aa02fdc73468ee2d4956cf8f0fc4fa40fa'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='f94ab4af-ddb2-484b-b65e-2b3fa6ac171e', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='23c263072d643a45b0eaa0c00ce0de6a5a29a2121f06853a497fdd81432e81a7')}, text='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\nsteps (dev) (dev)×106\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n(A)1 512 512 5.29 24.9\n4 128 128 5.00 25.5\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n(B)16 5.16 25.1 58\n32 5.01 25.4 60\n(C)2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n(D)0.0 5.77 24.6\n0.2 4.95 25.5\n0.0 4.67 25.3\n0.2 5.47 25.7\n(E) positional embedding instead of sinusoids 4.92 25.7\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\nresults to the base model.\n7 Conclusion\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor .\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\n9', start_char_idx=0, end_char_idx=2609, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.5964249020769608)]))]
